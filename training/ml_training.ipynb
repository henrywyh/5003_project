{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import RandomForestClassifier, RandomForestClassificationModel\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_path = './data/sentiment_tweets3.csv'\n",
    "reddit_path = './data/depression_dataset_reddit_cleaned.csv'\n",
    "mental_path = './data/mental_health.csv'\n",
    "\n",
    "# schema = StructType([StructField('text', StringType(), True),\n",
    "#                     StructField('label', IntegerType(), True)])\n",
    "\n",
    "df_twitter = spark.read.csv(twitter_path, header=True, inferSchema=True)\n",
    "df_reddit = spark.read.csv(reddit_path, header=True, inferSchema=True)\n",
    "df_mental = spark.read.csv(mental_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Index: integer (nullable = true)\n",
      " |-- message to examine: string (nullable = true)\n",
      " |-- label (depression result): string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- clean_text: string (nullable = true)\n",
      " |-- is_depression: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- text: string (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_twitter.printSchema()\n",
    "df_reddit.printSchema()\n",
    "df_mental.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter = df_twitter.withColumnRenamed(\"message to examine\", \"text\")\n",
    "df_twitter = df_twitter.withColumnRenamed(\"label (depression result)\", \"label\")\n",
    "df_twitter = df_twitter.withColumn(\"label\", df_twitter[\"label\"].cast(IntegerType()))\n",
    "df_twitter = df_twitter.drop('Index')\n",
    "\n",
    "df_reddit = df_reddit.withColumnRenamed(\"clean_text\", \"text\")\n",
    "df_reddit = df_reddit.withColumnRenamed(\"is_depression\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                text|label|\n",
      "+--------------------+-----+\n",
      "|just had a real g...|    0|\n",
      "|is reading manga ...|    0|\n",
      "|@comeagainjen htt...|    0|\n",
      "|@lapcat Need to s...|    0|\n",
      "|ADD ME ON MYSPACE...|    0|\n",
      "|so sleepy. good t...|    0|\n",
      "|@SilkCharm re: #n...|    0|\n",
      "|23 or 24ï¿½C poss...|    0|\n",
      "|nite twitterville...|    0|\n",
      "|@daNanner Night, ...|    0|\n",
      "|Good morning ever...|    0|\n",
      "|Finally! I just c...|    0|\n",
      "|kisha they cnt ge...|    0|\n",
      "|@nicolerichie Yes...|    0|\n",
      "|I really love ref...|    0|\n",
      "|@blueaero ooo it'...|    0|\n",
      "|@rokchic28 no pro...|    0|\n",
      "|@shipovalov &quot...|    0|\n",
      "|Once again stayed...|    0|\n",
      "|@Kal_Penn I just ...|    0|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+-----+\n",
      "|                text|label|\n",
      "+--------------------+-----+\n",
      "|we understand tha...|    1|\n",
      "|welcome to r depr...|    1|\n",
      "|anyone else inste...|    1|\n",
      "|i ve kind of stuf...|    1|\n",
      "|sleep is my great...|    1|\n",
      "|i m year old turn...|    1|\n",
      "|i live alone and ...|    1|\n",
      "|i m not looking f...|    1|\n",
      "|i don t know how ...|    1|\n",
      "|mom i m sad it hu...|    1|\n",
      "|i ve been struggl...|    1|\n",
      "|idk how to elabor...|    1|\n",
      "|i tried to help h...|    1|\n",
      "|to me it seems li...|    1|\n",
      "|my father committ...|    1|\n",
      "|i don t think i h...|    1|\n",
      "|tw suicide yea so...|    1|\n",
      "|got no one to tal...|    1|\n",
      "|i m sitting on my...|    1|\n",
      "|i hate myself so ...|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+-----+\n",
      "|                text|label|\n",
      "+--------------------+-----+\n",
      "|dear american tee...|    0|\n",
      "|nothing look forw...|    1|\n",
      "|music recommendat...|    0|\n",
      "|im done trying fe...|    1|\n",
      "|worried  year old...|    1|\n",
      "|hey rredflag sure...|    1|\n",
      "|feel like someone...|    0|\n",
      "|deserve liveif di...|    1|\n",
      "|feels good ive se...|    1|\n",
      "|live guiltok made...|    1|\n",
      "|excercise motivat...|    0|\n",
      "|know youd rather ...|    0|\n",
      "|even time fuck  s...|    0|\n",
      "|usual hollywood s...|    0|\n",
      "|think it nearly u...|    0|\n",
      "|trying rd time k ...|    0|\n",
      "|guy coming sure w...|    0|\n",
      "|one best episodes...|    0|\n",
      "|good byehey you k...|    1|\n",
      "|tried put sugar c...|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_twitter.show()\n",
    "df_reddit.show()\n",
    "df_mental.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "9\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(df_twitter.toPandas()['text'].isnull().sum())\n",
    "print(df_reddit.toPandas()['text'].isnull().sum())\n",
    "print(df_mental.toPandas()['text'].isnull().sum())\n",
    "\n",
    "print(df_twitter.toPandas()['label'].isnull().sum())\n",
    "print(df_reddit.toPandas()['label'].isnull().sum())\n",
    "print(df_mental.toPandas()['label'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter = df_twitter.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(df_twitter.toPandas()['text'].isnull().sum())\n",
    "print(df_reddit.toPandas()['text'].isnull().sum())\n",
    "print(df_mental.toPandas()['text'].isnull().sum())\n",
    "\n",
    "print(df_twitter.toPandas()['label'].isnull().sum())\n",
    "print(df_reddit.toPandas()['label'].isnull().sum())\n",
    "print(df_mental.toPandas()['label'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    1|19974|\n",
      "|    0|26039|\n",
      "+-----+-----+\n",
      "\n",
      "46013\n"
     ]
    }
   ],
   "source": [
    "df = df_twitter.union(df_reddit).union(df_mental)\n",
    "\n",
    "df.groupby('label').count().show()\n",
    "print(df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 37021 rows in the training set, and 8992 in the test set\n"
     ]
    }
   ],
   "source": [
    "trainDF, testDF = df.randomSplit([.8, .2], seed=42)\n",
    "print(f\"\"\"There are {trainDF.count()} rows in the training set, and {testDF.count()} in the test set\"\"\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "locale = sc._jvm.java.util.Locale\n",
    "locale.setDefault(locale.forLanguageTag(\"en-US\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol='text', outputCol='tokens')\n",
    "stopwords_remover = StopWordsRemover(inputCol='tokens', outputCol='filtered_tokens',locale='en_US.utf8')\n",
    "vectorizer = CountVectorizer(inputCol='filtered_tokens', outputCol='vectors')\n",
    "idf = IDF(inputCol='vectors', outputCol='features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(featuresCol='features', labelCol='label')\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, stopwords_remover, vectorizer, idf, lr])\n",
    "\n",
    "pipelineModel = pipeline.fit(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predDF = pipelineModel.transform(testDF)\n",
    "\n",
    "predDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label', metricName='accuracy')\n",
    "\n",
    "accuracy = evaluator.evaluate(predDF)\n",
    "print(f'Accuracy for logistic regression: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model_path = \"../backend/models/lr_model\"\n",
    "pipelineModel.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "loaded_pipeline_model  = PipelineModel.load(\"../backend/models/lr_model\")\n",
    "df = pipelineModel.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol='label', maxBins=40, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = (ParamGridBuilder()\n",
    "            .addGrid(rf.maxDepth, [2, 4, 6])\n",
    "            .addGrid(rf.numTrees, [10, 100])\n",
    "            .build())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label', metricName='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CrossValidator(estimator=rf,\n",
    "                    evaluator=evaluator,\n",
    "                    estimatorParamMaps=paramGrid,\n",
    "                    numFolds=3,\n",
    "                    parallelism=4,\n",
    "                    seed=42)\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, stopwords_remover, vectorizer, idf, rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "pipelineModel = pipeline.fit(trainDF)\n",
    "print('Time spent:', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predDF = pipelineModel.transform(testDF)\n",
    "accuracy = evaluator.evaluate(predDF)\n",
    "print(f'Accuracy for random forest: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
